{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercício 1\n",
    "\n",
    "Agora que já estamos um pouco familiarizados com as bibliotecas Requests e BeautifulSoup, vamos fazer o primeiro exercício dessa aula. Tente integrar as duas bibliotecas para podermos trabalhar com o Web Scraping propriamente dito. Ou seja, inserir o conteúdo da gerado do Requests da URL no BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     "TL; DR"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://pixabay.com/pt/photos/'\n",
    "payload = {'q': 'gatinhos felizes'}\n",
    "\n",
    "r = requests.get(url, params=payload)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercício 2\n",
    "\n",
    "Vamos aproveitar que já estamos nos sites de imagens de gatinhos, e baixar as 5 primeiras imagens de gatinhos. Nunca se sabe quando você vai precisar de imagens de gatinhos fofos na sua vida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://pixabay.com/pt/photos/'\n",
    "payload = {'q': 'gatinhos felizes'}\n",
    "\n",
    "r = requests.get(url, params=payload)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "divs = soup.find('div', {'id': 'photo_grid'}).contents\n",
    "i = 0\n",
    "while i <= 3:\n",
    "    img = divs[i].a.img\n",
    "    if img.get('data-lazy'):\n",
    "        img_url = img.get('data-lazy')\n",
    "    elif img.get('src'):\n",
    "        img_url = img.get('src')\n",
    "    \n",
    "    response = requests.get(img_url, stream=True)\n",
    "    \n",
    "    f = open('%d.png' % i, 'wb')\n",
    "    shutil.copyfileobj(response.raw, f)\n",
    "    f.close()\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercício 3\n",
    "\n",
    "Imagino que todo mundo já encomendou alguma coisa pela internet. Eu gosto muito de uma interface bem limpa para receber encomenda, ou seja, só me importa aonde a encomenda tá agora, tanto faz o que já aconteceu com ela. A Correios disponibilizou uma API pública para realizar esse tipo de consulta, então vamos usufruir dela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/03/2017 11:57 CEE AGUA BRANCA - Sao Paulo/SP Entrega Efetuada\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://websro.correios.com.br/sro_bin/txect01$.QueryList'\n",
    "payload = {\n",
    "    'P_LINGUA': '001',\n",
    "    'P_TIPO': '001',\n",
    "    'P_COD_UNI': 'OB035615101BR',\n",
    "}\n",
    "\n",
    "r = requests.get(url, params=payload)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "situacao = soup.find_all('tr')[1]\n",
    "hora, local, estado = situacao.find_all('td')\n",
    "print(hora.string, local.string, estado.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercício 4\n",
    "\n",
    "Agora para o nível magia negra do Python. Quero que instalem a biblioteca youtube_dl nos seus computadores e baixem um vídeo no Youtube. Para quem não conseguir achar como usa a youtube_dl ou tem muita preguiça, é isso (mostra o código). Só que como essa aula é de Web Scraping, então vou dificultar um pouco. Quero que vocês baixem imaginando que vocês resolveram procurar um vídeo na search bar do Youtube para baixar o vídeo que vocês querem (geralmente é o primeiro). Façam o código desde a url de busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import youtube_dl\n",
    "\n",
    "url = 'https://www.youtube.com'\n",
    "payload = {\n",
    "    'search_query': 'shape of you',\n",
    "}\n",
    "\n",
    "r = requests.get(url+'/results', params=payload)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "tag = soup.find('a', {'rel': 'spf-prefetch'})\n",
    "video_url = url + tag['href']\n",
    "\n",
    "ydl_opts = {}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([video_url])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
